import streamlit as st
import google.generativeai as genai
import PyPDF2
import os
import time
import pandas as pd
from datetime import datetime
import json

# ==========================================
# í˜ì´ì§€ ì„¤ì •
# ==========================================
st.set_page_config(
    page_title="ë³´í—˜ ì•½ê´€ ë¹„êµ ë¶„ì„ AI", 
    page_icon="âš–ï¸", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì»¤ìŠ¤í…€ CSSë¡œ UI ê°œì„ 
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .subtitle {
        text-align: center;
        color: #666;
        margin-bottom: 2rem;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 24px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding: 0 24px;
    }
    .upload-box {
        border: 2px dashed #1f77b4;
        border-radius: 10px;
        padding: 20px;
        text-align: center;
        background-color: #f0f8ff;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 20px;
        border-radius: 10px;
        color: white;
        text-align: center;
    }
    </style>
""", unsafe_allow_html=True)

# ==========================================
# í—¤ë”
# ==========================================
st.markdown('<div class="main-header">âš–ï¸ ë³´í—˜ ì•½ê´€ ë¹„êµ ë¶„ì„ AI</div>', unsafe_allow_html=True)
st.markdown('<div class="subtitle">ì—¬ëŸ¬ ë³´í—˜ì‚¬ì˜ ì•½ê´€ì„ í•œ ë²ˆì— ë¹„êµí•˜ê³  í•µì‹¬ ë³´ì¥ ë‚´ìš©ì„ ë¶„ì„í•©ë‹ˆë‹¤</div>', unsafe_allow_html=True)

# ==========================================
# API í‚¤ ì„¤ì •
# ==========================================
@st.cache_resource
def configure_api():
    """API í‚¤ ì„¤ì • (ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ)"""
    try:
        if "GEMINI_API_KEY" in st.secrets:
            genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
            return True
        else:
            st.error("ğŸ”‘ Secretsì— GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            st.info("ğŸ’¡ Streamlit Cloudì—ì„œ Settings > Secretsì— API í‚¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
            return False
    except Exception as e:
        st.error(f"âŒ API í‚¤ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
        return False

if not configure_api():
    st.stop()

# ==========================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==========================================

@st.cache_data(show_spinner=False)
def extract_text_from_pdf(file_bytes, filename):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìºì‹± ì ìš©)"""
    try:
        from io import BytesIO
        pdf_file = BytesIO(file_bytes)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        text = ""
        total_pages = len(pdf_reader.pages)
        
        for i, page in enumerate(pdf_reader.pages):
            extracted = page.extract_text()
            if extracted:
                text += extracted + "\n"
        
        return text, total_pages, None
    except Exception as e:
        return "", 0, str(e)

def get_smart_context(full_text, query, max_chunks=15):
    """
    ìŠ¤ë§ˆíŠ¸ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ê°œì„ ëœ ë²„ì „)
    - í‚¤ì›Œë“œ ë§¤ì¹­ ê°•í™”
    - TF-IDF ìŠ¤íƒ€ì¼ ìŠ¤ì½”ì–´ë§
    """
    if not full_text or not query:
        return ""
    
    chunk_size = 2500  # ë” í° ì²­í¬ë¡œ ì»¨í…ìŠ¤íŠ¸ í–¥ìƒ
    overlap = 500  # ì¤‘ë³µ ì˜ì—­ ì¶”ê°€
    
    chunks = []
    for i in range(0, len(full_text), chunk_size - overlap):
        chunk = full_text[i:i+chunk_size]
        if chunk.strip():
            chunks.append(chunk)
    
    # ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬
    query_keywords = [word.lower() for word in query.split() if len(word) > 1]
    
    scored_chunks = []
    for chunk in chunks:
        chunk_lower = chunk.lower()
        score = 0
        
        # í‚¤ì›Œë“œë³„ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        for keyword in query_keywords:
            count = chunk_lower.count(keyword)
            # ë¹ˆë„ê°€ ë†’ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            score += count * (1 + len(keyword) / 10)
        
        if score > 0:
            scored_chunks.append((score, chunk))
    
    # ì ìˆ˜ìˆœ ì •ë ¬
    scored_chunks.sort(key=lambda x: x[0], reverse=True)
    
    # ìƒìœ„ ì²­í¬ ì„ íƒ
    top_chunks = [chunk for score, chunk in scored_chunks[:max_chunks]]
    
    return "\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n".join(top_chunks)

def generate_ai_response(prompt, temperature=0.3):
    """
    AI ì‘ë‹µ ìƒì„± (í´ë°± ëª¨ë¸ ì§€ì›)
    """
    candidate_models = [
        "gemini-2.0-flash-exp",      # ìµœì‹  ëª¨ë¸ ìš°ì„ 
        "gemini-1.5-flash",          
        "gemini-1.5-flash-001",      
        "gemini-flash-latest"        
    ]
    
    generation_config = {
        "temperature": temperature,
        "top_p": 0.95,
        "top_k": 40,
        "max_output_tokens": 8192,
    }
    
    last_error = None
    for model_name in candidate_models:
        try:
            model = genai.GenerativeModel(
                model_name=model_name,
                generation_config=generation_config
            )
            response = model.generate_content(prompt)
            
            # ì•ˆì „ í•„í„° ì²´í¬
            if hasattr(response, 'prompt_feedback'):
                if response.prompt_feedback.block_reason:
                    continue
            
            return response.text, model_name
            
        except Exception as e:
            last_error = e
            continue
    
    raise Exception(f"ëª¨ë“  ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {str(last_error)}")

def create_comparison_prompt(context, question, file_names):
    """
    ë¹„êµ ë¶„ì„ì„ ìœ„í•œ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
    """
    prompt = f"""
ë‹¹ì‹ ì€ **ë³´í—˜ ì•½ê´€ ë¶„ì„ ì „ë¬¸ê°€**ì…ë‹ˆë‹¤.

ğŸ“‹ **ë¶„ì„ ëŒ€ìƒ íŒŒì¼ë“¤**
{', '.join(file_names)}

ğŸ“š **ì œê³µëœ ì•½ê´€ ë‚´ìš©**
{context}

â“ **ì‚¬ìš©ì ì§ˆë¬¸**
{question}

ğŸ“Š **ë‹µë³€ í˜•ì‹ ìš”êµ¬ì‚¬í•­**

1. **ë¹„êµ í‘œ ì‘ì„± í•„ìˆ˜**
   - ë§ˆí¬ë‹¤ìš´ í‘œ(Markdown Table) í˜•ì‹ ì‚¬ìš©
   - ì—´ êµ¬ì„±: `í•­ëª©` | `ë³´í—˜ì‚¬1` | `ë³´í—˜ì‚¬2` | `ë³´í—˜ì‚¬3` | `ë¹„ê³ `
   - ê° ì…€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±

2. **í•µì‹¬ ì°¨ì´ì  ê°•ì¡°**
   - ë³´ì¥ ê¸ˆì•¡, ë³´ì¥ ë²”ìœ„, íŠ¹ì•½ ë“±ì˜ ì°¨ì´ë¥¼ ëª…í™•íˆ í‘œì‹œ
   - ì¤‘ìš”í•œ ì°¨ì´ëŠ” **êµµì€ ê¸€ì”¨**ë¡œ ê°•ì¡°

3. **ì •í™•ì„± ìš°ì„ **
   - ì•½ê´€ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ ê²ƒ
   - ë¶ˆëª…í™•í•œ ë¶€ë¶„ì€ "ì•½ê´€ì— ëª…ì‹œ ì•ˆ ë¨" í‘œê¸°

4. **ì¶”ê°€ ë¶„ì„**
   - í‘œ ì•„ë˜ì— í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 3ê°€ì§€ ìš”ì•½
   - ì†Œë¹„ì ê´€ì ì—ì„œ ì£¼ì˜í•  ì  ì–¸ê¸‰

5. **ì‹œê°ì  êµ¬ì¡°í™”**
   - ì´ëª¨ì§€ í™œìš©ìœ¼ë¡œ ê°€ë…ì„± í–¥ìƒ
   - ì„¹ì…˜ë³„ êµ¬ë¶„ ëª…í™•íˆ

ë‹µë³€ì„ ì‹œì‘í•˜ì„¸ìš”:
"""
    return prompt

# ==========================================
# ì‚¬ì´ë“œë°” - íŒŒì¼ ì—…ë¡œë“œ
# ==========================================
with st.sidebar:
    st.header("ğŸ“‚ ì•½ê´€ íŒŒì¼ ì—…ë¡œë“œ")
    
    st.markdown("""
    <div class="upload-box">
        <p>ğŸ’¼ ì—¬ëŸ¬ ë³´í—˜ì‚¬ ì•½ê´€ì„ í•œ ë²ˆì— ì„ íƒí•˜ì„¸ìš”</p>
        <small>PDF ë˜ëŠ” TXT íŒŒì¼ ì§€ì›</small>
    </div>
    """, unsafe_allow_html=True)
    
    uploaded_files = st.file_uploader(
        "íŒŒì¼ ì„ íƒ",
        type=['pdf', 'txt'],
        accept_multiple_files=True,
        label_visibility="collapsed"
    )
    
    if uploaded_files:
        st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")
        
        # íŒŒì¼ ì •ë³´ í‘œì‹œ
        with st.expander("ğŸ“„ ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡", expanded=True):
            for idx, file in enumerate(uploaded_files, 1):
                file_size = len(file.getvalue()) / 1024  # KB
                st.write(f"{idx}. **{file.name}** ({file_size:.1f} KB)")
    
    st.divider()
    
    # ë¶„ì„ ì˜µì…˜
    st.subheader("âš™ï¸ ë¶„ì„ ì˜µì…˜")
    analysis_depth = st.select_slider(
        "ë¶„ì„ ê¹Šì´",
        options=["ë¹ ë¥¸ ë¶„ì„", "í‘œì¤€", "ìƒì„¸ ë¶„ì„"],
        value="í‘œì¤€"
    )
    
    include_recommendations = st.checkbox("ğŸ’¡ ì¶”ì²œ ì‚¬í•­ í¬í•¨", value=True)
    
    st.divider()
    
    # ì‚¬ìš© ê°€ì´ë“œ
    with st.expander("ğŸ“– ì‚¬ìš© ê°€ì´ë“œ"):
        st.markdown("""
        **1ë‹¨ê³„**: ì™¼ìª½ì—ì„œ ì•½ê´€ íŒŒì¼ ì—…ë¡œë“œ
        
        **2ë‹¨ê³„**: ì±„íŒ…ì°½ì— ì§ˆë¬¸ ì…ë ¥
        - ì˜ˆ: "ì•” ì§„ë‹¨ê¸ˆ ë¹„êµí•´ì¤˜"
        - ì˜ˆ: "ìˆ˜ìˆ ë¹„ ì°¨ì´ë¥¼ í‘œë¡œ ë³´ì—¬ì¤˜"
        
        **3ë‹¨ê³„**: AI ë¶„ì„ ê²°ê³¼ í™•ì¸
        
        **íŒ**: êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ìŠµë‹ˆë‹¤!
        """)

# ==========================================
# ë©”ì¸ ì˜ì—­
# ==========================================

if not uploaded_files:
    # íŒŒì¼ì´ ì—†ì„ ë•Œ ì•ˆë‚´ í™”ë©´
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ì•½ê´€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”")
        
        st.markdown("### ğŸ¯ ì£¼ìš” ê¸°ëŠ¥")
        features = {
            "ğŸ“Š ìë™ ë¹„êµ í‘œ": "ì—¬ëŸ¬ ì•½ê´€ì„ í•œëˆˆì— ë¹„êµ",
            "ğŸ¤– AI ë¶„ì„": "í•µì‹¬ ì°¨ì´ì  ìë™ ì¶”ì¶œ",
            "ğŸ’¬ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤": "ê¶ê¸ˆí•œ ì ì„ ììœ ë¡­ê²Œ ì§ˆë¬¸",
            "ğŸ“ˆ ì‹œê°í™”": "ë³µì¡í•œ ì •ë³´ë¥¼ ì‰½ê²Œ ì´í•´"
        }
        
        for feature, desc in features.items():
            st.markdown(f"**{feature}**: {desc}")
        
        st.markdown("### ğŸ’¡ ì§ˆë¬¸ ì˜ˆì‹œ")
        example_questions = [
            "ì•” ì§„ë‹¨ê¸ˆê³¼ ìˆ˜ìˆ ë¹„ë¥¼ ë³´í—˜ì‚¬ë³„ë¡œ ë¹„êµí•´ì¤˜",
            "ê°±ì‹ í˜•ê³¼ ë¹„ê°±ì‹ í˜•ì˜ ì°¨ì´ê°€ ë­ì•¼?",
            "íŠ¹ì•½ ë‚´ìš©ì„ í‘œë¡œ ì •ë¦¬í•´ì¤˜",
            "ë³´ì¥ ì œì™¸ í•­ëª©ì€ ì–´ë–¤ ê²Œ ìˆì–´?"
        ]
        
        for q in example_questions:
            st.code(q, language=None)
    
    st.stop()

# ==========================================
# íŒŒì¼ ì²˜ë¦¬ ë° ë¶„ì„
# ==========================================

# ì§„í–‰ ìƒíƒœ í‘œì‹œ
progress_bar = st.progress(0)
status_text = st.empty()

combined_text = ""
file_names = []
file_stats = []

# íŒŒì¼ ì½ê¸°
status_text.text("ğŸ“„ íŒŒì¼ì„ ì½ëŠ” ì¤‘...")
for idx, uploaded_file in enumerate(uploaded_files):
    progress = (idx + 1) / len(uploaded_files)
    progress_bar.progress(progress)
    
    file_names.append(uploaded_file.name)
    content = ""
    pages = 0
    error = None
    
    try:
        if uploaded_file.name.endswith(".pdf"):
            file_bytes = uploaded_file.getvalue()
            content, pages, error = extract_text_from_pdf(file_bytes, uploaded_file.name)
        else:
            content = uploaded_file.read().decode("utf-8")
            pages = len(content.split('\n'))
        
        if error:
            st.warning(f"âš ï¸ {uploaded_file.name}: {error}")
        
        # íŒŒì¼ë³„ êµ¬ë¶„ì ì¶”ê°€
        combined_text += f"\n\n{'='*50}\n"
        combined_text += f"[íŒŒì¼: {uploaded_file.name}]\n"
        combined_text += f"{'='*50}\n\n"
        combined_text += content
        
        file_stats.append({
            "íŒŒì¼ëª…": uploaded_file.name,
            "í˜ì´ì§€/ì¤„": pages,
            "í¬ê¸°": f"{len(uploaded_file.getvalue()) / 1024:.1f} KB",
            "ê¸€ììˆ˜": len(content)
        })
        
    except Exception as e:
        st.error(f"âŒ {uploaded_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

progress_bar.empty()
status_text.empty()

# í†µê³„ í‘œì‹œ
if file_stats:
    st.success("âœ… ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ğŸ“ íŒŒì¼ ìˆ˜", len(file_stats))
    with col2:
        total_chars = sum(stat["ê¸€ììˆ˜"] for stat in file_stats)
        st.metric("ğŸ“ ì´ ê¸€ì ìˆ˜", f"{total_chars:,}")
    with col3:
        total_size = sum(float(stat["í¬ê¸°"].replace(" KB", "")) for stat in file_stats)
        st.metric("ğŸ’¾ ì´ í¬ê¸°", f"{total_size:.1f} KB")
    with col4:
        st.metric("ğŸ¯ ë¶„ì„ ì¤€ë¹„", "ì™„ë£Œ")
    
    # ìƒì„¸ í†µê³„ (ì ‘ê¸° ê°€ëŠ¥)
    with st.expander("ğŸ“Š íŒŒì¼ë³„ ìƒì„¸ ì •ë³´"):
        df_stats = pd.DataFrame(file_stats)
        st.dataframe(df_stats, use_container_width=True)

st.divider()

# ==========================================
# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
# ==========================================

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
    
    welcome_msg = f"""
ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹

ì´ **{len(file_names)}ê°œì˜ ì•½ê´€**ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤:
{chr(10).join([f"â€¢ {name}" for name in file_names])}

ê¶ê¸ˆí•˜ì‹  ë‚´ìš©ì„ ììœ ë¡­ê²Œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!

**ì¶”ì²œ ì§ˆë¬¸:**
- "ê° ë³´í—˜ì‚¬ì˜ ì•” ì§„ë‹¨ê¸ˆê³¼ ìˆ˜ìˆ ë¹„ë¥¼ ë¹„êµí•´ì¤˜"
- "íŠ¹ì•½ ë‚´ìš©ì˜ ì°¨ì´ì ì„ í‘œë¡œ ë³´ì—¬ì¤˜"
- "ë³´ì¥ ì œì™¸ í•­ëª©ì€ ë­ê°€ ìˆì–´?"
"""
    st.session_state.messages.append({
        "role": "assistant",
        "content": welcome_msg
    })

# ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (ì˜ˆ: ì•” ì§„ë‹¨ê¸ˆ ë¹„êµí•´ì¤˜)"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        msg_placeholder = st.empty()
        msg_placeholder.markdown("ğŸ” ì•½ê´€ì„ ë¶„ì„í•˜ëŠ” ì¤‘...")
        
        try:
            # ë¶„ì„ ê¹Šì´ì— ë”°ë¥¸ ì²­í¬ ìˆ˜ ì¡°ì •
            chunk_map = {
                "ë¹ ë¥¸ ë¶„ì„": 8,
                "í‘œì¤€": 15,
                "ìƒì„¸ ë¶„ì„": 25
            }
            max_chunks = chunk_map.get(analysis_depth, 15)
            
            # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
            with st.spinner("ğŸ“š ê´€ë ¨ ë‚´ìš©ì„ ì°¾ëŠ” ì¤‘..."):
                relevant_context = get_smart_context(
                    combined_text, 
                    prompt, 
                    max_chunks=max_chunks
                )
            
            if not relevant_context.strip():
                msg_placeholder.warning("âš ï¸ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
                st.stop()
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            analysis_prompt = create_comparison_prompt(
                relevant_context,
                prompt,
                file_names
            )
            
            # AI ì‘ë‹µ ìƒì„±
            start_time = time.time()
            response_text, model_used = generate_ai_response(analysis_prompt)
            elapsed_time = time.time() - start_time
            
            # ì‘ë‹µ í‘œì‹œ
            msg_placeholder.markdown(response_text)
            
            # ë©”íƒ€ ì •ë³´
            col1, col2, col3 = st.columns(3)
            with col1:
                st.caption(f"âš¡ ëª¨ë¸: {model_used}")
            with col2:
                st.caption(f"â±ï¸ ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
            with col3:
                st.caption(f"ğŸ“ ë¶„ì„ ê¹Šì´: {analysis_depth}")
            
            # ì¶”ì²œ ì‚¬í•­ ì¶”ê°€
            if include_recommendations and "ì¶”ì²œ" not in prompt.lower():
                with st.expander("ğŸ’¡ AI ì¶”ì²œ ì‚¬í•­"):
                    st.info("ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!")
            
            # ë©”ì‹œì§€ ì €ì¥
            st.session_state.messages.append({
                "role": "assistant",
                "content": response_text
            })
            
        except Exception as e:
            msg_placeholder.error("âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")
            st.error(f"ì˜¤ë¥˜ ì„¸ë¶€ì •ë³´: {str(e)}")
            
            # ì—ëŸ¬ ë¡œê¹…
            if st.session_state.get("debug_mode", False):
                st.exception(e)

# ==========================================
# í‘¸í„°
# ==========================================
st.divider()
col1, col2, col3 = st.columns([2, 3, 2])
with col2:
    st.caption("âš–ï¸ ë³´í—˜ ì•½ê´€ ë¹„êµ ë¶„ì„ AI | Powered by Google Gemini")
    st.caption("âš ï¸ ë³¸ ë¶„ì„ì€ ì°¸ê³ ìš©ì´ë©°, ìµœì¢… ê²°ì • ì‹œ ì•½ê´€ ì›ë¬¸ì„ í™•ì¸í•˜ì„¸ìš”")


import streamlit as st
import google.generativeai as genai
import PyPDF2
import os
import time

# ==========================================
# [ì„¤ì •] ë°±ê³¼ì‚¬ì „ íŒŒì¼ ëª©ë¡
BOOK_PARTS = [
    "jsbgocrc1.pdf",
    "jsbgocrc2.pdf",
    "jsbgocrc3.pdf",
    "jsbgocrc4.pdf"
]
# ==========================================

st.set_page_config(page_title="í™ˆ ë‹¥í„° AI", page_icon="ğŸ¥", layout="wide")
st.title("ğŸ¥ ë‚´ ì†ì•ˆì˜ ì£¼ì¹˜ì˜ (Premium)")

# 1. í‚¤ ì„¤ì •
try:
    if "GEMINI_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
    else:
        st.error("ë¹„ë°€ ê¸ˆê³ ì— í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
except:
    st.error("í‚¤ ì„¤ì • ì˜¤ë¥˜")
    st.stop()

# 2. ë°ì´í„° í†µí•© í•¨ìˆ˜
@st.cache_resource
def load_and_merge_books(file_list):
    full_text = ""
    status_text = st.empty()
    try:
        valid_files = [f for f in file_list if os.path.exists(f)]
        if not valid_files:
            return None

        status_text.info("ğŸ“š ë°±ê³¼ì‚¬ì „ ë°ì´í„°ë¥¼ í†µí•©í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        for filename in valid_files:
            with open(filename, "rb") as f:
                pdf_reader = PyPDF2.PdfReader(f)
                for page in pdf_reader.pages:
                    extracted = page.extract_text()
                    if extracted:
                        full_text += extracted + "\n"
        
        status_text.success(f"âœ… ë°±ê³¼ì‚¬ì „ ì¤€ë¹„ ì™„ë£Œ! (ì´ {len(full_text)}ì)")
        return full_text
    except Exception as e:
        status_text.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# 3. ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ í•¨ìˆ˜ (ìœ ë£Œë‹ˆê¹Œ ë„‰ë„‰í•˜ê²Œ 10ê°œ!)
def get_relevant_content(full_text, query):
    chunk_size = 1000
    chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]
    relevant_chunks = []
    query_keywords = query.split()
    
    for chunk in chunks:
        score = 0
        for word in query_keywords:
            if word in chunk:
                score += 1
        if score > 0:
            relevant_chunks.append((score, chunk))
    
    relevant_chunks.sort(key=lambda x: x[0], reverse=True)
    # ìœ ë£Œ íšŒì›ì´ì‹œë‹ˆ ì •ë³´ë¥¼ ë” ë§ì´(10ê°œ) ë´…ë‹ˆë‹¤.
    top_chunks = [chunk for score, chunk in relevant_chunks[:10]]
    return "\n...\n".join(top_chunks)

# 4. [í•µì‹¬] ë§ŒëŠ¥ ìë™ ì ‘ì† í•¨ìˆ˜ (ì•Œì•„ì„œ ì°¾ì•„ëƒ„)
def generate_with_auto_selection(prompt):
    # ì‹œë„í•  ëª¨ë¸ ìˆœì„œ (ì„±ëŠ¥ ì¢‹ê³  ì•ˆì •ì ì¸ ìˆœì„œ)
    candidate_models = [
        "gemini-1.5-flash",          # 1ìˆœìœ„: ê°€ì¥ í‘œì¤€ì ì´ê³  ë¹ ë¦„
        "gemini-1.5-flash-001",      # 2ìˆœìœ„: êµ¬ë²„ì „ (ì•ˆì •ì„± ç”²)
        "gemini-2.0-flash-lite",     # 3ìˆœìœ„: ì‹ í˜• ë¼ì´íŠ¸
        "gemini-flash-latest"        # 4ìˆœìœ„: ìµœí›„ì˜ ë³´ë£¨
    ]
    
    last_error = None
    
    for model_name in candidate_models:
        try:
            # ì ‘ì† ì‹œë„
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            return response.text, model_name # ì„±ê³µ ì‹œ ë‚´ìš©ê³¼ ëª¨ë¸ëª… ë°˜í™˜
            
        except Exception as e:
            last_error = e
            # ì‹¤íŒ¨í•˜ë©´ ë‹¤ìŒ ëª¨ë¸ë¡œ ì¡°ìš©íˆ ë„˜ì–´ê°
            continue 

    # ëª¨ë“  ëª¨ë¸ì´ ë‹¤ ì‹¤íŒ¨í–ˆì„ ë•Œë§Œ ì—ëŸ¬ ë¿œìŒ
    raise Exception(f"ëª¨ë“  ì ‘ì† ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì—ëŸ¬: {str(last_error)}")

# 5. UI ë° ë¡œì§
with st.sidebar:
    st.header("ğŸ“‚ ìë£Œ ë“±ë¡")
    uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ (PDF/TXT)", type=['pdf', 'txt'])
    st.info(f"ê¸°ë³¸ íƒ‘ì¬: ë°±ê³¼ì‚¬ì „ (ì´ {len(BOOK_PARTS)}ê¶Œ)")

encyclopedia_text = load_and_merge_books(BOOK_PARTS)
target_text = ""
use_smart_search = False

if uploaded_file:
    try:
        if uploaded_file.name.endswith(".pdf"):
            pdf_reader = PyPDF2.PdfReader(uploaded_file)
            for page in pdf_reader.pages:
                extracted = page.extract_text()
                if extracted:
                    target_text += extracted + "\n"
        else:
            target_text = uploaded_file.read().decode("utf-8")
    except Exception as e:
        st.error(f"ì½ê¸° ì‹¤íŒ¨: {str(e)}")
        st.stop()
        
    if len(target_text) > 30000:
        use_smart_search = True
        st.toast("ğŸš€ ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ê°€ë™ (Premium)")
else:
    if encyclopedia_text:
        target_text = encyclopedia_text
        use_smart_search = True
    else:
        st.error("ë°±ê³¼ì‚¬ì „ íŒŒì¼ ì—†ìŒ")
        st.stop()

# 6. ì±„íŒ…ì°½
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."})

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ì¦ìƒì„ ì…ë ¥í•˜ì„¸ìš”"):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        msg_placeholder = st.empty()
        msg_placeholder.markdown("ğŸ” ë¶„ì„ ì¤‘...")
        
        try:
            if use_smart_search:
                final_context = get_relevant_content(target_text, prompt)
                if not final_context or len(final_context.strip()) == 0:
                    final_context = "ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                final_context = target_text

            full_prompt = f"""
            ë¬¸ì„œ ë‚´ìš©:
            {final_context}
            
            ì§ˆë¬¸: {prompt}
            
            ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            """
            
            # [ìë™ ì ‘ì† ì‹¤í–‰]
            final_response, used_model = generate_with_auto_selection(full_prompt)
            
            msg_placeholder.markdown(final_response)
            st.session_state.messages.append({"role": "assistant", "content": final_response})
            
            # ì—°ê²°ëœ ëª¨ë¸ ì´ë¦„ í‘œì‹œ (ì„±ê³µ í™•ì¸ìš©)
            st.caption(f"âš¡ Connected to: {used_model}")
            
        except Exception as e:
            st.error("âŒ ì—°ê²° ì‹¤íŒ¨")
            st.error(f"ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")
            st.warning("âš ï¸ ìœ ë£Œ ê²°ì œí•œ í”„ë¡œì íŠ¸ì˜ API í‚¤ê°€ Secretsì— ì •í™•íˆ ë“¤ì–´ê°”ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
