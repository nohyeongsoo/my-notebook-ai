import streamlit as st
import google.generativeai as genai
import PyPDF2

# 1. ì•± ì œëª© ì„¤ì •
st.set_page_config(page_title="ë‚˜ë§Œì˜ ë…¸íŠ¸ë¶LM", page_icon="ğŸ¤–")
st.title("ğŸ¤– ë‚´ ë…¸íŠ¸ë¶ AI ë¹„ì„œ (í…ìŠ¤íŠ¸ ëª¨ë“œ)")
st.write("PDF íŒŒì¼ì„ ì˜¬ë¦¬ë©´, ë‚´ìš©ì„ ë¶„ì„í•´ ë‹µë³€í•´ì¤ë‹ˆë‹¤.")

# 2. ì‚¬ì´ë“œë°”: API í‚¤ ì…ë ¥ë°›ê¸°
with st.sidebar:
    api_key = st.text_input("Gemini API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    st.markdown("---")
    st.markdown("Google AI Studioì—ì„œ ë°œê¸‰ë°›ì€ í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# 3. íŒŒì¼ ì—…ë¡œë“œ ê¸°ëŠ¥
uploaded_file = st.file_uploader("í•™ìŠµì‹œí‚¬ PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type=['pdf'])

# 4. ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# 5. ì±„íŒ… í™”ë©´ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 6. ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬
if prompt := st.chat_input("ë‚´ìš©ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!"):
    if not api_key:
        st.error("ì™¼ìª½ ì‚¬ì´ë“œë°”ì— API Keyë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”!")
        st.stop()

    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        try:
            genai.configure(api_key=api_key)
            
            # === [í•µì‹¬] í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ ê°€ì¥ ì•ˆì „í•œ ëª¨ë¸ ì‚¬ìš© ===
            # ëª©ë¡ì— ìˆë˜ ê²ƒ ì¤‘ ê°€ì¥ ë¬´ë‚œí•œ ëª¨ë¸
            model = genai.GenerativeModel('gemini-flash-latest') 
            
            context = ""
            if uploaded_file:
                # [ìš°íšŒë²•] íŒŒì¼ì„ êµ¬ê¸€ì— ì•ˆ ì˜¬ë¦¬ê³ , ì—¬ê¸°ì„œ ì§ì ‘ ê¸€ìë¥¼ ëºë‹ˆë‹¤.
                pdf_reader = PyPDF2.PdfReader(uploaded_file)
                for page in pdf_reader.pages:
                    context += page.extract_text() + "\n"
                
                # AIì—ê²Œ ì¤„ í¸ì§€(í”„ë¡¬í”„íŠ¸) ì™„ì„±
                full_prompt = f"""
                ë‹¤ìŒì€ ë¬¸ì„œì˜ ë‚´ìš©ì…ë‹ˆë‹¤:
                {context}
                
                ì‚¬ìš©ìì˜ ì§ˆë¬¸: {prompt}
                
                ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
                """
                
                response = model.generate_content(full_prompt)
            else:
                # íŒŒì¼ì´ ì—†ì„ ë•ŒëŠ” ê·¸ëƒ¥ ì§ˆë¬¸ë§Œ
                response = model.generate_content(prompt)

            full_response = response.text
            message_placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})

        except Exception as e:
            st.error(f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")