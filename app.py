import streamlit as st
import google.generativeai as genai
import PyPDF2
import os

# ==========================================
# [ì„¤ì •] ìª¼ê°œì„œ ì˜¬ë¦° íŒŒì¼ ì´ë¦„ë“¤ (ì±…ì¥ ëª©ë¡)
# ì„ ìƒë‹˜ì´ ì˜¬ë¦¬ì‹  íŒŒì¼ëª…ìœ¼ë¡œ ì •í™•íˆ ì ì–´ì£¼ì„¸ìš”!
BOOK_PARTS = [
    "book1.pdf",
    "book2.pdf",
    "book3.pdf",
    "book4.pdf"
    # í•„ìš”í•œ ë§Œí¼ íŒŒì¼ ì´ë¦„ì„ ê³„ì† ì¶”ê°€í•˜ì„¸ìš” (ì½¤ë§ˆ ì£¼ì˜!)
]
# ==========================================

st.set_page_config(page_title="í™ˆ ë‹¥í„° AI", page_icon="ğŸ¥")
st.title("ğŸ¥ ë‚´ ì†ì•ˆì˜ ì£¼ì¹˜ì˜ (ì¦ìƒ ë°±ê³¼ì‚¬ì „)")

# 1. í‚¤ ì„¤ì •
try:
    if "GEMINI_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
    else:
        st.error("ë¹„ë°€ ê¸ˆê³ ì— í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
except:
    st.error("í‚¤ ì„¤ì • ì˜¤ë¥˜")
    st.stop()

# 2. [ìˆ˜ì •ë¨] ì œí•œ ì—†ì´ ëê¹Œì§€ ì½ëŠ” í•¨ìˆ˜
@st.cache_resource
def load_and_merge_books(file_list):
    full_text = ""
    total_pages_read = 0
    
    # ì§„í–‰ ìƒí™© í‘œì‹œ
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    try:
        total_files = len(file_list)
        
        for idx, filename in enumerate(file_list):
            if not os.path.exists(filename):
                continue
            
            status_text.info(f"ğŸ“š {idx+1}ë²ˆì§¸ ì±…({filename})ì„ ì½ëŠ” ì¤‘...")
            
            with open(filename, "rb") as f:
                pdf_reader = PyPDF2.PdfReader(f)
                num_pages = len(pdf_reader.pages)
                
                # [í•µì‹¬] í˜ì´ì§€ ì œí•œ ì—†ì´ forë¬¸ì´ ëê¹Œì§€ ë•ë‹ˆë‹¤!
                for page in pdf_reader.pages:
                    extracted = page.extract_text()
                    if extracted:
                        full_text += extracted + "\n"
                
                total_pages_read += num_pages
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress_bar.progress((idx + 1) / total_files)

        status_text.success(f"âœ… ë°±ê³¼ì‚¬ì „ ì™„ì „ ì •ë³µ! (ì´ {total_pages_read}í˜ì´ì§€)")
        progress_bar.empty()
        return full_text

    except Exception as e:
        status_text.error(f"ì±…ì„ ì½ëŠ” ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return None

# 3. ì‹¤í–‰ ë¡œì§
if not any(os.path.exists(f) for f in BOOK_PARTS):
    st.error("âš ï¸ GitHubì— ì—…ë¡œë“œëœ ì±… íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. BOOK_PARTS ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# ì±… í•©ì²´ ë° ë¡œë“œ
encyclopedia_text = load_and_merge_books(BOOK_PARTS)

if not encyclopedia_text:
    st.stop()

# 4. ì±„íŒ… í™”ë©´
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({"role": "assistant", "content": "ì–´ë””ê°€ ë¶ˆí¸í•˜ì‹ ê°€ìš”? 712í˜ì´ì§€ ì „ì²´ ë‚´ìš©ì„ ê²€ìƒ‰í•´ ë“œë¦´ê²Œìš”."})

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ì¦ìƒì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ëª…ì¹˜ ìª½ì´ ë‹µë‹µí•´ìš”)"):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        msg_placeholder = st.empty()
        msg_placeholder.markdown("ğŸ” ì „ì²´ ë°±ê³¼ì‚¬ì „ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")
        
        try:
            # 2.5 ëª¨ë¸ (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ìš©)
            model = genai.GenerativeModel('gemini-2.5-flash')
            
            full_prompt = f"""
            ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ì˜í•™ ìƒë‹´ AIì…ë‹ˆë‹¤.
            ì•„ë˜ [ë°±ê³¼ì‚¬ì „ í†µí•©ë³¸] ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

            [ë°±ê³¼ì‚¬ì „ í†µí•©ë³¸]
            {encyclopedia_text}
            
            [ì‚¬ìš©ì ì¦ìƒ]
            {prompt}
            
            ê·œì¹™:
            1. ë°±ê³¼ì‚¬ì „ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            2. ê´€ë ¨ëœ ì˜í•™ ì •ë³´ë¥¼ ì°¾ì•„ì„œ ì›ì¸ê³¼ ëŒ€ì²˜ë²•ì„ ì„¤ëª…í•˜ì„¸ìš”.
            """
            
            response = model.generate_content(full_prompt)
            msg_placeholder.markdown(response.text)
            st.session_state.messages.append({"role": "assistant", "content": response.text})
            
        except Exception as e:
            # ì§„ì§œ êµ¬ê¸€ í•œë„ ì´ˆê³¼ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€
            if "429" in str(e):
                msg_placeholder.error("âš ï¸ ë‚´ìš©ì´ ë„ˆë¬´ ë°©ëŒ€í•˜ì—¬ êµ¬ê¸€ ì„œë²„ê°€ ì ì‹œ ìˆ¨ì„ ê³ ë¥´ê³  ìˆìŠµë‹ˆë‹¤. (1ë¶„ ë’¤ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”)")
            else:
                msg_placeholder.error(f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")













