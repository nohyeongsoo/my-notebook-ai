import streamlit as st
import google.generativeai as genai
import PyPDF2
import os

# ==========================================
# [ì„¤ì •] ìª¼ê°œì„œ ì˜¬ë¦° ë°±ê³¼ì‚¬ì „ íŒŒì¼ ëª©ë¡
# (GitHubì— ì˜¬ë¦° íŒŒì¼ ì´ë¦„ê³¼ ë˜‘ê°™ì´ ì ì–´ì£¼ì„¸ìš”)
BOOK_PARTS = [
    "jsbgocrc1.pdf",
    "jsbgocrc2.pdf",
    "jsbgocrc3.pdf",
    "jsbgocrc4.pdf"
]
# ==========================================

st.set_page_config(page_title="í™ˆ ë‹¥í„° AI", page_icon="ğŸ¥", layout="wide")
st.title("ğŸ¥ ë‚´ ì†ì•ˆì˜ ì£¼ì¹˜ì˜ (ì¦ìƒ ë°±ê³¼ì‚¬ì „)")

# 1. í‚¤ ì„¤ì •
try:
    if "GEMINI_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
    else:
        st.error("ë¹„ë°€ ê¸ˆê³ ì— í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
except:
    st.error("í‚¤ ì„¤ì • ì˜¤ë¥˜")
    st.stop()

# 2. ì±… ì½ê¸° í•¨ìˆ˜ (í•œ ë²ˆë§Œ ì‹¤í–‰)
@st.cache_resource
def load_and_merge_books(file_list):
    full_text = ""
    status_text = st.empty()
    try:
        # íŒŒì¼ì´ í•˜ë‚˜ë¼ë„ ìˆëŠ”ì§€ í™•ì¸
        valid_files = [f for f in file_list if os.path.exists(f)]
        if not valid_files:
            return None

        status_text.info("ğŸ“š ë°±ê³¼ì‚¬ì „ ë°ì´í„°ë¥¼ í†µí•©í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        for filename in valid_files:
            with open(filename, "rb") as f:
                pdf_reader = PyPDF2.PdfReader(f)
                for page in pdf_reader.pages:
                    extracted = page.extract_text()
                    if extracted:
                        full_text += extracted + "\n"
        
        status_text.success(f"âœ… ë°±ê³¼ì‚¬ì „ ì¤€ë¹„ ì™„ë£Œ! (ì´ {len(full_text)}ì)")
        return full_text
    except Exception as e:
        status_text.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# 3. [í•µì‹¬ ê¸°ìˆ ] ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ í•¨ìˆ˜ (ì—ëŸ¬ í•´ê²°ì‚¬!)
# ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ë½‘ì•„ëƒ…ë‹ˆë‹¤.
def get_relevant_content(full_text, query):
    # ë³¸ë¬¸ì„ 1000ì ë‹¨ìœ„ë¡œ ìª¼ê°­ë‹ˆë‹¤
    chunk_size = 1000
    chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]
    
    relevant_chunks = []
    query_keywords = query.split() # ì§ˆë¬¸ì„ ë‹¨ì–´ë¡œ ìª¼ê°¬
    
    for chunk in chunks:
        # ì§ˆë¬¸ì˜ ë‹¨ì–´ê°€ í¬í•¨ëœ ë¶€ë¶„ë§Œ ì°¾ìŒ
        score = 0
        for word in query_keywords:
            if word in chunk:
                score += 1
        
        if score > 0:
            relevant_chunks.append((score, chunk))
    
    # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ìˆœì„œë¡œ ì •ë ¬í•´ì„œ ìƒìœ„ 10ê°œë§Œ ë½‘ìŒ (ì•½ 1ë§Œ ì)
    # ì´ë ‡ê²Œ í•˜ë©´ AIì—ê²Œ ë³´ë‚´ëŠ” ì–‘ì´ í™• ì¤„ì–´ì„œ ì—ëŸ¬ê°€ ì•ˆ ë‚©ë‹ˆë‹¤!
    relevant_chunks.sort(key=lambda x: x[0], reverse=True)
    top_chunks = [chunk for score, chunk in relevant_chunks[:15]]
    
    return "\n...\n".join(top_chunks)

# 4. ì‚¬ì´ë“œë°” (íŒŒì¼ ì—…ë¡œë“œ ê¸°ëŠ¥ ì¶”ê°€)
with st.sidebar:
    st.header("ğŸ“‚ ì¶”ê°€ ìë£Œ ë“±ë¡")
    st.write("ë°±ê³¼ì‚¬ì „ ì™¸ì— ë¶„ì„í•  íŒŒì¼ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì˜¬ë¦¬ì„¸ìš”.")
    uploaded_file = st.file_uploader("ê°œì¸ ì˜ë£Œê¸°ë¡ ë“± (PDF/TXT)", type=['pdf', 'txt'])
    
    st.write("---")
    st.info(f"ê¸°ë³¸ íƒ‘ì¬: ë°±ê³¼ì‚¬ì „ (ì´ {len(BOOK_PARTS)}ê¶Œ)")

# 5. ë°ì´í„° ë¡œë“œ ë¡œì§
encyclopedia_text = load_and_merge_books(BOOK_PARTS)
target_text = ""
source_info = ""

# ì‚¬ìš©ìê°€ íŒŒì¼ì„ ì˜¬ë ¸ìœ¼ë©´ ê·¸ê±¸ ìš°ì„ ìœ¼ë¡œ, ì•„ë‹ˆë©´ ë°±ê³¼ì‚¬ì „ì„ ì‚¬ìš©
if uploaded_file:
    # ì—…ë¡œë“œ íŒŒì¼ ì½ê¸°
    if uploaded_file.name.endswith(".pdf"):
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        for page in pdf_reader.pages:
            target_text += page.extract_text() + "\n"
    else:
        target_text = uploaded_file.read().decode("utf-8")
    source_info = f"ğŸ“‚ ì—…ë¡œë“œëœ íŒŒì¼ ({uploaded_file.name})"
    use_smart_search = False # ì—…ë¡œë“œ íŒŒì¼ì€ ì§§ìœ¼ë‹ˆê¹Œ ì „ì²´ ë¶„ì„
else:
    # ë°±ê³¼ì‚¬ì „ ì‚¬ìš©
    if encyclopedia_text:
        target_text = encyclopedia_text
        source_info = "ğŸ“• ì¦ìƒ ë°±ê³¼ì‚¬ì „ (ì „ì²´)"
        use_smart_search = True # ë°±ê³¼ì‚¬ì „ì€ ë„ˆë¬´ í¬ë‹ˆê¹Œ ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ì‚¬ìš©!
    else:
        st.error("ë°±ê³¼ì‚¬ì „ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. GitHubë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

# 6. ì±„íŒ… í™”ë©´
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”. ì¦ìƒì„ ë§ì”€í•´ ì£¼ì‹œë©´ ë°±ê³¼ì‚¬ì „ì—ì„œ ì°¾ì•„ ì•Œë ¤ë“œë¦´ê²Œìš”."})

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ì¦ìƒì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì—´ì´ ë‚˜ê³  ì˜¤í•œì´ ìˆì–´ìš”)"):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        msg_placeholder = st.empty()
        msg_placeholder.markdown("ğŸ” ê´€ë ¨ ë‚´ìš©ì„ ì°¾ëŠ” ì¤‘...")
        
        try:
            # [ì¤‘ìš”] ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ì ìš©
            if use_smart_search:
                # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¶€ë¶„ë§Œ ì™ ë½‘ì•„ì„œ AIì—ê²Œ ì¤ë‹ˆë‹¤.
                final_context = get_relevant_content(target_text, prompt)
                if not final_context:
                    final_context = "ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    msg_placeholder.info("ì°¸ê³ : ë°±ê³¼ì‚¬ì „ì—ì„œ ì •í™•í•œ í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¦ìƒì„ ë” ìì„¸íˆ ì ì–´ë³´ì„¸ìš”.")
            else:
                final_context = target_text

            # 2.5 ëª¨ë¸ ì‚¬ìš©
            model = genai.GenerativeModel('gemini-2.5-flash')
            
            full_prompt = f"""
            ë‹¹ì‹ ì€ ì˜í•™ ìƒë‹´ AIì…ë‹ˆë‹¤.
            ì•„ë˜ [ë¬¸ì„œ ë‚´ìš©]ì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.

            [ë¬¸ì„œ ë‚´ìš© (ë°œì·Œ)]
            {final_context}
            
            [ì‚¬ìš©ì ì¦ìƒ]
            {prompt}
            
            ê·œì¹™:
            1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì— ìˆëŠ” ì •ë³´ë§Œìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ë©´ "ì±…ì—ì„œ í•´ë‹¹ ì¦ìƒì— ëŒ€í•œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”.
            3. ì „ë¬¸ì ì´ê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
            """
            
            response = model.generate_content(full_prompt)
            msg_placeholder.markdown(response.text)
            st.session_state.messages.append({"role": "assistant", "content": response.text})
            
        except Exception as e:
            msg_placeholder.error("âš ï¸ ì ì‹œ ì—°ê²°ì´ ì›í™œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”)")















