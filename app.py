import streamlit as st
import google.generativeai as genai
import PyPDF2
import os

# ==========================================
# [ì„¤ì •] ìª¼ê°œì„œ ì˜¬ë¦° íŒŒì¼ ì´ë¦„ë“¤ì„ ì—¬ê¸°ì— ë‹¤ ì ì–´ì£¼ì„¸ìš”!
# ì•±ì´ ì´ ìˆœì„œëŒ€ë¡œ ì½ì–´ì„œ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
BOOK_PARTS = [
    "jsbgocrc1.pdf",
    "jsbgocrc2.pdf",
    "jsbgocrc3.pdf",
    "jsbgocrc4.pdf" 
    # í•„ìš”í•œ ë§Œí¼ ê³„ì† ì¶”ê°€í•˜ì„¸ìš” (ì½¤ë§ˆ ì£¼ì˜!)
]
# ==========================================

st.set_page_config(page_title="í™ˆ ë‹¥í„° AI", page_icon="ğŸ¥")
st.title("ğŸ¥ ë‚´ ì†ì•ˆì˜ ì£¼ì¹˜ì˜ (ì¦ìƒ ë°±ê³¼ì‚¬ì „)")

# 1. í‚¤ ì„¤ì •
try:
    if "GEMINI_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
    else:
        st.error("ë¹„ë°€ ê¸ˆê³ ì— í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
except:
    st.error("í‚¤ ì„¤ì • ì˜¤ë¥˜")
    st.stop()

# 2. ì—¬ëŸ¬ íŒŒì¼ì„ ì½ì–´ì„œ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜
@st.cache_resource
def load_and_merge_books(file_list):
    full_text = ""
    total_pages_read = 0
    
    # ì§„í–‰ ìƒí™©ì„ ë³´ì—¬ì¤„ ë¹ˆì¹¸
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    try:
        total_files = len(file_list)
        
        for idx, filename in enumerate(file_list):
            if not os.path.exists(filename):
                continue # íŒŒì¼ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
            
            status_text.info(f"ğŸ“š {idx+1}ë²ˆì§¸ ì±…({filename})ì„ ì½ê³  í•©ì¹˜ëŠ” ì¤‘...")
            
            with open(filename, "rb") as f:
                pdf_reader = PyPDF2.PdfReader(f)
                # ê° ì±…ì˜ í˜ì´ì§€ë¥¼ ë‹¤ ì½ìŒ
                for page in pdf_reader.pages:
                    extracted = page.extract_text()
                    if extracted:
                        full_text += extracted + "\n"
                
                total_pages_read += len(pdf_reader.pages)
            
            # ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
            progress_bar.progress((idx + 1) / total_files)

        # ë‹¤ ì½ì—ˆìœ¼ë©´ ì •ë¦¬
        status_text.success(f"âœ… ì´ {total_pages_read}í˜ì´ì§€ ë¶„ëŸ‰ì˜ ë°±ê³¼ì‚¬ì „ í•™ìŠµ ì™„ë£Œ!")
        progress_bar.empty() # ì§„í–‰ë°” ìˆ¨ê¹€
        return full_text

    except Exception as e:
        status_text.error(f"ì±…ì„ ì½ëŠ” ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return None

# 3. ì‹¤í–‰ ë¡œì§
# íŒŒì¼ë“¤ì´ í•˜ë‚˜ë¼ë„ ìˆëŠ”ì§€ í™•ì¸
if not any(os.path.exists(f) for f in BOOK_PARTS):
    st.error("âš ï¸ GitHubì— ì—…ë¡œë“œëœ ì±… íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# í•©ì²´ ì‹œì‘!
encyclopedia_text = load_and_merge_books(BOOK_PARTS)

if not encyclopedia_text:
    st.stop()

# 4. ì±„íŒ… í™”ë©´
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({"role": "assistant", "content": "ì–´ë””ê°€ ë¶ˆí¸í•˜ì‹ ê°€ìš”? ì¦ìƒì„ ë§ì”€í•´ ì£¼ì„¸ìš”. ë°±ê³¼ì‚¬ì „ ì „ì²´ë¥¼ ê²€ìƒ‰í•´ ë“œë¦´ê²Œìš”."})

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ì¦ìƒì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì˜¤ë¥¸ìª½ ë°°ê°€ ì½•ì½• ì‘¤ì…”ìš”)"):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        msg_placeholder = st.empty()
        msg_placeholder.markdown("ğŸ” 720í˜ì´ì§€ ì „ì²´ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")
        
        try:
            # 2.5 ëª¨ë¸ (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ì— ê°•í•¨)
            model = genai.GenerativeModel('gemini-2.5-flash')
            
            full_prompt = f"""
            ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ì˜í•™ ìƒë‹´ AIì…ë‹ˆë‹¤.
            ì•„ë˜ [ë°±ê³¼ì‚¬ì „ í†µí•©ë³¸]ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì¦ìƒì„ ë¶„ì„í•˜ì„¸ìš”.

            [ë°±ê³¼ì‚¬ì „ í†µí•©ë³¸ ë‚´ìš©]
            {encyclopedia_text}
            
            [ì‚¬ìš©ì ì¦ìƒ]
            {prompt}
            
            ë‹µë³€ ì‹œ ì£¼ì˜ì‚¬í•­:
            1. ë°±ê³¼ì‚¬ì „ì— ìˆëŠ” ë‚´ìš©ì— ê·¼ê±°í•´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
            2. ì¶”ì¸¡í•˜ì§€ ë§ê³  ì±…ì— ìˆëŠ” íŒ©íŠ¸ë¥¼ ì „ë‹¬í•˜ì„¸ìš”.
            3. ì‹¬ê°í•´ ë³´ì´ë©´ ë³‘ì›ì— ê°€ë³´ë¼ëŠ” ì¡°ì–¸ì„ ë§ë¶™ì´ì„¸ìš”.
            """
            
            response = model.generate_content(full_prompt)
            msg_placeholder.markdown(response.text)
            st.session_state.messages.append({"role": "assistant", "content": response.text})
            
        except Exception as e:
            msg_placeholder.error("âš ï¸ ë‚´ìš©ì´ ë„ˆë¬´ ë°©ëŒ€í•˜ì—¬ ì²˜ë¦¬ê°€ ì§€ì—°ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜, ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”.")












