import streamlit as st
import google.generativeai as genai
import PyPDF2
import os
import time

# ==========================================
# [ì„¤ì •] ë°±ê³¼ì‚¬ì „ íŒŒì¼ ëª©ë¡
BOOK_PARTS = [
    "jsbgocrc1.pdf",
    "jsbgocrc2.pdf",
    "jsbgocrc3.pdf",
    "jsbgocrc4.pdf"
]
# ==========================================

st.set_page_config(page_title="í™ˆ ë‹¥í„° AI", page_icon="ğŸ¥", layout="wide")
st.title("ğŸ¥ ë‚´ ì†ì•ˆì˜ ì£¼ì¹˜ì˜ (ë§ŒëŠ¥ ì ‘ì† ë²„ì „)")

# 1. í‚¤ ì„¤ì •
try:
    if "GEMINI_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
    else:
        st.error("ë¹„ë°€ ê¸ˆê³ ì— í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
except:
    st.error("í‚¤ ì„¤ì • ì˜¤ë¥˜")
    st.stop()

# 2. ë°ì´í„° í†µí•© í•¨ìˆ˜
@st.cache_resource
def load_and_merge_books(file_list):
    full_text = ""
    status_text = st.empty()
    try:
        valid_files = [f for f in file_list if os.path.exists(f)]
        if not valid_files:
            return None

        status_text.info("ğŸ“š ë°±ê³¼ì‚¬ì „ ë°ì´í„°ë¥¼ í†µí•©í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        for filename in valid_files:
            with open(filename, "rb") as f:
                pdf_reader = PyPDF2.PdfReader(f)
                for page in pdf_reader.pages:
                    extracted = page.extract_text()
                    if extracted:
                        full_text += extracted + "\n"
        
        status_text.success(f"âœ… ë°±ê³¼ì‚¬ì „ ì¤€ë¹„ ì™„ë£Œ! (ì´ {len(full_text)}ì)")
        return full_text
    except Exception as e:
        status_text.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# 3. ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ í•¨ìˆ˜ (ê°€ë³ê²Œ 5ê°œë§Œ ì¶”ì¶œ)
def get_relevant_content(full_text, query):
    chunk_size = 1000
    chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]
    relevant_chunks = []
    query_keywords = query.split()
    
    for chunk in chunks:
        score = 0
        for word in query_keywords:
            if word in chunk:
                score += 1
        if score > 0:
            relevant_chunks.append((score, chunk))
    
    relevant_chunks.sort(key=lambda x: x[0], reverse=True)
    top_chunks = [chunk for score, chunk in relevant_chunks[:5]]
    return "\n...\n".join(top_chunks)

# 4. [í•µì‹¬] ë§ŒëŠ¥ ì ‘ì† ì‹œë„ í•¨ìˆ˜ (ìˆœì„œëŒ€ë¡œ ë‹¤ ì°”ëŸ¬ë´„)
def generate_with_auto_model_selection(prompt):
    # ì‹œë„í•´ë³¼ ëª¨ë¸ ëª©ë¡ (ìš°ì„ ìˆœìœ„: ì œí•œì´ ë„ë„í•œ 1.5 ì‹œë¦¬ì¦ˆ)
    candidate_models = [
        "gemini-1.5-flash",          # 1ìˆœìœ„: ê°€ì¥ í‘œì¤€ì ì¸ ë¬´ì œí•œ ëª¨ë¸
        "gemini-1.5-flash-001",      # 2ìˆœìœ„: êµ¬ë²„ì „ (ì•ˆì •ì )
        "gemini-1.5-flash-002",      # 3ìˆœìœ„: ì‹ ë²„ì „
        "gemini-1.5-flash-latest",   # 4ìˆœìœ„: ìµœì‹  ë³„ì¹­
        "gemini-flash-latest"        # 5ìˆœìœ„: ìµœí›„ì˜ ìˆ˜ë‹¨
    ]
    
    last_error = ""
    
    for model_name in candidate_models:
        try:
            # ëª¨ë¸ ìƒì„± ì‹œë„
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            return response.text, model_name # ì„±ê³µí•˜ë©´ ë‚´ìš©ê³¼ ëª¨ë¸ëª… ë°˜í™˜
            
        except Exception as e:
            error_msg = str(e)
            # 429(ì œí•œì´ˆê³¼)ë‚˜ 404(ëª¨ë¸ì—†ìŒ)ë©´ ë‹¤ìŒ ëª¨ë¸ë¡œ ë„˜ì–´ê°
            last_error = error_msg
            continue 

    # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í–ˆì„ ë•Œ
    raise Exception(f"ëª¨ë“  ëª¨ë¸ ì ‘ì† ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì—ëŸ¬: {last_error}")

# 5. UI ë° ë¡œì§
with st.sidebar:
    st.header("ğŸ“‚ ìë£Œ ë“±ë¡")
    uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ (PDF/TXT)", type=['pdf', 'txt'])
    st.info(f"ê¸°ë³¸ íƒ‘ì¬: ë°±ê³¼ì‚¬ì „ (ì´ {len(BOOK_PARTS)}ê¶Œ)")

encyclopedia_text = load_and_merge_books(BOOK_PARTS)
target_text = ""
use_smart_search = False

if uploaded_file:
    try:
        if uploaded_file.name.endswith(".pdf"):
            pdf_reader = PyPDF2.PdfReader(uploaded_file)
            for page in pdf_reader.pages:
                extracted = page.extract_text()
                if extracted:
                    target_text += extracted + "\n"
        else:
            target_text = uploaded_file.read().decode("utf-8")
    except Exception as e:
        st.error(f"ì½ê¸° ì‹¤íŒ¨: {str(e)}")
        st.stop()
        
    if len(target_text) > 30000:
        use_smart_search = True
        st.toast("ğŸš€ ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ê°€ë™")
else:
    if encyclopedia_text:
        target_text = encyclopedia_text
        use_smart_search = True
    else:
        st.error("ë°±ê³¼ì‚¬ì „ íŒŒì¼ ì—†ìŒ")
        st.stop()

# 6. ì±„íŒ…ì°½
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”. ì¦ìƒì„ ë§ì”€í•´ ì£¼ì„¸ìš”. (ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤)"})

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ì¦ìƒì„ ì…ë ¥í•˜ì„¸ìš”"):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        msg_placeholder = st.empty()
        msg_placeholder.markdown("ğŸ” ì ‘ì† ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ëŠ” ì¤‘...")
        
        try:
            if use_smart_search:
                final_context = get_relevant_content(target_text, prompt)
                if not final_context or len(final_context.strip()) == 0:
                    final_context = "ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                final_context = target_text

            full_prompt = f"""
            ë¬¸ì„œ ë‚´ìš©:
            {final_context}
            
            ì§ˆë¬¸: {prompt}
            
            ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            """
            
            # [ìë™ ì°¾ê¸° ì‹¤í–‰]
            final_response, used_model = generate_with_auto_model_selection(full_prompt)
            
            msg_placeholder.markdown(final_response)
            st.session_state.messages.append({"role": "assistant", "content": final_response})
            
            # (ë””ë²„ê¹…ìš©) ì–´ë–¤ ëª¨ë¸ì´ ì„±ê³µí–ˆëŠ”ì§€ ì‘ê²Œ í‘œì‹œ
            st.caption(f"âš¡ ì—°ê²°ëœ ëª¨ë¸: {used_model}")
            
        except Exception as e:
            st.error("âŒ ëª¨ë“  ì—°ê²° ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            st.error(f"ì—ëŸ¬ ë‚´ìš©: {str(e)}")
            if "429" in str(e):
                st.warning("âš ï¸ í˜„ì¬ ëª¨ë“  ëª¨ë¸ì˜ ì‚¬ìš©ëŸ‰ì´ ê½‰ ì°¼ìŠµë‹ˆë‹¤. ë‚´ì¼ ë‹¤ì‹œ ì‹œë„í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.")


























